{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4623073a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention mechanisms in transformers allow the model to focus on different parts of the input sequence when generating each word of the output sequence.  It's like highlighting relevant words in a sentence when trying to understand its meaning.  Here's a breakdown:\n",
      "\n",
      "1. **Queries, Keys, and Values:**  Imagine you're searching for information (query) in a database (keys and values).  Each word in the input sequence is transformed into three vectors:\n",
      "\n",
      "    * **Query (Q):** Represents what the current word is \"looking for\" in other words.\n",
      "    * **Key (K):** Represents what each word \"offers\" in terms of information.\n",
      "    * **Value (V):** Represents the actual information content of each word.\n",
      "\n",
      "    These vectors are created by multiplying the input embeddings by three different weight matrices (W<sub>Q</sub>, W<sub>K</sub>, W<sub>V</sub>), which are learned during training.\n",
      "\n",
      "2. **Calculating Attention Weights:** The core of attention is calculating the relevance of each word in the input sequence to the current word being processed. This is done by comparing the query vector of the current word with the key vectors of all other words (including itself).  A common method is the scaled dot-product attention:\n",
      "\n",
      "    * **Dot Product:** The query vector is multiplied (dot product) with each key vector.  This measures the similarity between the query and each key.  Higher dot product means higher similarity.\n",
      "    * **Scaling:** The dot product is divided by the square root of the key vector's dimension (âˆšd<sub>k</sub>). This prevents the dot products from becoming too large, which can lead to instability during training.\n",
      "    * **Softmax:** A softmax function is applied to the scaled dot products. This normalizes the scores into probabilities, representing the attention weights.  These weights sum up to 1, indicating the relative importance of each word.\n",
      "\n",
      "3. **Weighted Average of Values:** The attention weights are then used to create a weighted average of the value vectors.  This weighted average represents the context vector for the current word, incorporating information from relevant parts of the input sequence.\n",
      "\n",
      "    * Each value vector is multiplied by its corresponding attention weight.\n",
      "    * The resulting weighted vectors are summed together to create the context vector.\n",
      "\n",
      "4. **Output:** This context vector, rich with information from relevant parts of the input, is then used in further processing steps of the transformer, ultimately contributing to the generation of the output sequence.\n",
      "\n",
      "**Multi-Head Attention:**\n",
      "\n",
      "Transformers often employ multi-head attention. This involves performing the attention mechanism multiple times with different learned weight matrices (W<sub>Q</sub>, W<sub>K</sub>, W<sub>V</sub>) for each \"head.\"  Each head focuses on different aspects of the input sequence. The outputs from all heads are then concatenated and linearly transformed to produce the final context vector.  This allows the model to capture richer relationships between words.\n",
      "\n",
      "**Example:**\n",
      "\n",
      "Consider the sentence \"The cat sat on the mat.\" When processing the word \"sat,\" the attention mechanism might assign higher weights to \"cat\" and \"mat,\" as these words are contextually relevant. The context vector for \"sat\" would then be a weighted average of the value vectors for all words, with \"cat\" and \"mat\" contributing more significantly.\n",
      "\n",
      "\n",
      "In summary, attention allows transformers to dynamically weigh the importance of different parts of the input sequence when processing each word, enabling them to capture complex dependencies and relationships between words. This is a key factor in their powerful performance on various natural language processing tasks.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "chat = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "response = chat.invoke(\"Explain how attention works in transformers\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0729e256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.5-flash-preview-04-17\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/learnlm-2.0-flash-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n",
      "models/gemini-2.0-flash-live-001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nuthankishoremaddineni/Desktop/Github_Project/GitLearn/LangChainEnv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure the API\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "# List available models\n",
    "models = genai.list_models()\n",
    "for model in models:\n",
    "    print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fb48382",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiClient:\n",
    "    def __init__(self, model_name=\"gemini-1.5-pro\", api_key=None):\n",
    "        import google.generativeai as genai\n",
    "        self.genai = genai\n",
    "        self.genai.configure(api_key=api_key or os.environ[\"GOOGLE_API_KEY\"])\n",
    "        self.model = self.genai.GenerativeModel(model_name=model_name)\n",
    "\n",
    "    def chat(self, message):\n",
    "        response = self.model.generate_content(message)\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32104c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In deep learning, a transformer is a neural network architecture primarily known for its use in sequence-to-sequence tasks, like machine translation, text summarization, and question answering. Unlike recurrent neural networks (RNNs), transformers don't process sequential data in a step-by-step manner. Instead, they leverage a mechanism called **self-attention** to capture relationships between all elements in the input sequence simultaneously.  This allows for parallel processing, making transformers significantly faster than RNNs, especially for long sequences.\n",
      "\n",
      "Here's a breakdown of key components and concepts within a transformer:\n",
      "\n",
      "* **Self-Attention:** This is the core innovation of transformers.  It allows the model to weigh the importance of different parts of the input sequence when generating an output for a specific position.  Imagine reading a sentence; you don't just consider each word in isolation.  You relate words to each other to understand the overall meaning. Self-attention mimics this by allowing the model to consider the relationship between each word and all other words in the sentence when processing that word.\n",
      "\n",
      "* **Multi-Head Attention:** Instead of using just one self-attention mechanism, transformers use multiple \"heads\" of attention.  Each head focuses on different aspects of the input sequence, capturing a richer set of relationships.\n",
      "\n",
      "* **Encoder-Decoder Architecture:**  Transformers typically follow an encoder-decoder structure:\n",
      "    * **Encoder:** Processes the input sequence and generates a contextualized representation of it. This representation captures the meaning of the input sequence by considering the relationships between all its elements.  It consists of stacked identical layers, each containing multi-head self-attention and feed-forward networks.\n",
      "    * **Decoder:** Generates the output sequence based on the encoded representation of the input. It also uses multi-head self-attention to attend to different parts of the *output* sequence as it's being generated, and it attends to the encoder output to incorporate information from the input sequence. Like the encoder, it consists of stacked identical layers.\n",
      "\n",
      "* **Positional Encoding:** Since transformers don't process sequentially, they need a way to incorporate positional information about the words in the input sequence. This is done through positional encodings, which are added to the input embeddings.\n",
      "\n",
      "* **Feed-Forward Networks:**  Each encoder and decoder layer includes a feed-forward network that further processes the output of the attention mechanism.\n",
      "\n",
      "* **Layer Normalization:**  Used to stabilize training and improve performance.\n",
      "\n",
      "\n",
      "**Key Advantages of Transformers:**\n",
      "\n",
      "* **Parallelization:**  Can process sequences in parallel, unlike RNNs.\n",
      "* **Long-Range Dependencies:**  Effectively captures relationships between distant elements in a sequence.\n",
      "* **Scalability:**  Can be trained on massive datasets.\n",
      "\n",
      "\n",
      "**Examples of Transformer Models:**\n",
      "\n",
      "* **BERT (Bidirectional Encoder Representations from Transformers):** Used for various NLP tasks like text classification and question answering.\n",
      "* **GPT (Generative Pre-trained Transformer):**  Used for text generation and other tasks.\n",
      "* **Vision Transformer (ViT):** Adapts the transformer architecture for image recognition.\n",
      "\n",
      "\n",
      "In summary, transformers are powerful neural network architectures that have revolutionized the field of deep learning, particularly in natural language processing.  Their ability to process sequences in parallel and capture long-range dependencies makes them highly effective for a wide range of tasks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client = GeminiClient()\n",
    "print(client.chat(\"What is a transformer in deep learning?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9654438d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GeminiClient' object has no attribute 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m available_models = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m,\u001b[38;5;28mlist\u001b[39m()\n",
      "\u001b[31mAttributeError\u001b[39m: 'GeminiClient' object has no attribute 'models'"
     ]
    }
   ],
   "source": [
    "available_models = client.models,list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312a1ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChainEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
